# Stage 2: Speech to Voila token alignment
seed: 43
precision: bf16
train_steps: 100000
save_interval: 2000
log_interval: 50
validation_interval: 1000
resume_from: checkpoints/stage1/last

mixed_precision: bf16
use_deepspeed: true
accelerate_config: configs/deepspeed_zero3_bf16.json

optimizer:
  lr_adapter: 0.0005
  lr_audio_head: 0.001
  lr_qwen_unfrozen: 0.00005
  lr_whisper_unfrozen: 0.00001
  warmup_steps: 1000
  scheduler: cosine
  min_lr_ratio: 0.1

batching:
  max_tokens_per_device: 3200
  gradient_accumulation: 12
  dataloader_num_workers: 8
  max_context: 2048
  bucket_by_length: true

freeze:
  whisper:
    trainable_layers: [20, 21]
  qwen:
    frozen_blocks: [0, 27]
    lora_blocks: [20, 27]

loss:
  objectives:
    - name: audio_codebook_ce
      weight: 1.0
    - name: distill_kl
      weight: 0.2

w2c_distill:
  teacher_head_dim: 512
  temperature: 1.0
  loss_scale: 0.2

mixing:
  datasets:
    - type: asr
      weight: 0.7
    - type: tts
      weight: 0.3
  shuffle_buffer: 4096

validation_metrics:
  - codebook_accuracy
  - mel_mcd
