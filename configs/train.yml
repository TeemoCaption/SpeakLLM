stages:
  stage1:
    description: Stage 1 - Tiny ASR alignment (Whisper encoder → Qwen text頭)
    seed: 42
    precision: bf16
    train_steps: 16000
    save_interval: 2000
    log_interval: 50
    validation_interval: 800
    resume_from: null
    mixed_precision: bf16
    use_deepspeed: true
    accelerate_config: configs/deepspeed.json
    optimizer:
      lr_adapter: 0.001
      lr_text_head: 0.001
      lr_qwen_lora: 0.00005
      warmup_steps: 800
      scheduler: cosine
      min_lr_ratio: 0.2
    batching:
      max_tokens_per_device: 3072
      gradient_accumulation: 12
      dataloader_num_workers: 8
      max_context: 2048
      bucket_by_length: true
    train_manifests:
      - manifest: data/manifests/tiny_asr.jsonl
        weight: 1.0
        type: asr
    eval_manifests:
      - manifest: data/manifests/tiny_asr_dev.jsonl
        weight: 1.0
        type: asr
    data_fields:
      text: text
      prompt: prompt
      response: response
      audio_codes: audio_codes
      mel_path: mel_path
      chunk_spans: chunk_spans
      interrupt_label: interrupt_label
    freeze:
      whisper:
        trainable_layers: []
      qwen:
        use_lora: true
        frozen_blocks: [0, 27]
    loss:
      objectives:
        - name: text_ce
          weight: 1.0
        - name: ctc
          weight: 0.2
    ctc_head:
      enabled: true
      hidden_dim: 512
      dropout: 0.1
    spec_augment:
      enabled: true
      time_mask_param: 18
      time_mask_count: 2
      freq_mask_param: 12
      freq_mask_count: 2
    noise_augmentation:
      snr_db: [8, 20]
      probability: 0.35
      reverb_probability: 0.15
    curriculum:
      freeze_schedule:
        - step: 8000
          unfreeze_qwen_blocks: [24, 27]
    validation_metrics: [wer, cer]

  stage2:
    description: Stage 2 - Tiny speech token建模（文字→音訊 RVQ）
    seed: 43
    precision: bf16
    train_steps: 32000
    save_interval: 2000
    log_interval: 50
    validation_interval: 1000
    resume_from: checkpoints/stage1/last
    mixed_precision: bf16
    use_deepspeed: true
    accelerate_config: configs/deepspeed.json
    optimizer:
      lr_adapter: 0.001
      lr_audio_head: 0.001
      lr_qwen_lora: 0.00005
      warmup_steps: 1000
      scheduler: cosine
      min_lr_ratio: 0.2
    batching:
      max_tokens_per_device: 3072
      gradient_accumulation: 12
      dataloader_num_workers: 8
      max_context: 2048
      bucket_by_length: true
    train_manifests:
      - manifest: data/manifests/tiny_asr.jsonl
        weight: 0.4
        type: asr
      - manifest: data/manifests/tiny_tts.jsonl
        weight: 0.6
        type: tts
      - manifest: data/manifests/tiny_synth_tts.jsonl
        weight: 1.0
        type: synth
    eval_manifests:
      - manifest: data/manifests/tiny_tts_dev.jsonl
        weight: 1.0
        type: tts
    freeze:
      whisper:
        trainable_layers: [20]
      qwen:
        use_lora: true
        frozen_blocks: [0, 27]
    loss:
      objectives:
        - name: audio_codebook_ce
          weight: 1.0
        - name: distill_kl
          weight: 0.1
    w2c_distill:
      teacher_head_dim: 512
      temperature: 1.0
      loss_scale: 0.1
    mixing:
      datasets:
        - type: asr
          weight: 0.3
        - type: tts
          weight: 0.3
        - type: synth
          weight: 0.4
      shuffle_buffer: 2048
    validation_metrics: [codebook_accuracy, mel_mcd]

  stage4:
    description: Stage 3 - Tiny 聯合生成（文字 + 音訊 token）
    seed: 45
    precision: bf16
    train_steps: 52000
    save_interval: 3000
    log_interval: 50
    validation_interval: 1200
    resume_from: checkpoints/stage2/last
    use_deepspeed: true
    accelerate_config: configs/deepspeed.json
    optimizer:
      lr_adapter: 0.0001
      lr_audio_head: 0.0001
      lr_text_head: 0.0001
      lr_qwen_lora: 0.00002
      warmup_steps: 1500
      scheduler: cosine
      min_lr_ratio: 0.1
    batching:
      max_tokens_per_device: 3600
      gradient_accumulation: 16
      dataloader_num_workers: 8
      max_context: 4096
      bucket_by_length: true
    train_manifests:
      - manifest: data/manifests/tiny_asr.jsonl
        weight: 0.4
        type: asr
      - manifest: data/manifests/tiny_tts.jsonl
        weight: 0.3
        type: tts
      - manifest: data/manifests/tiny_synth_tts.jsonl
        weight: 0.6
        type: synth
      - manifest: data/manifests/tiny_dialog.jsonl
        weight: 0.3
        type: spoken_dialog
    eval_manifests:
      - manifest: data/manifests/tiny_dialog_dev.jsonl
        weight: 1.0
        type: spoken_dialog
    freeze:
      whisper:
        trainable_layers: [18, 23]
      qwen:
        use_lora: true
        frozen_blocks: []
    loss:
      objectives:
        - name: text_ce
          weight: 1.0
        - name: audio_codebook_ce
          weight: 1.0
    mixing:
      datasets:
        - name: spoken_dialog
          weight: 0.3
        - name: asr
          weight: 0.2
        - name: tts
          weight: 0.2
        - name: synth
          weight: 0.3
    validation_metrics: [perplexity, codebook_accuracy, mcd, wer]

  stage5:
    description: Stage 4 - Tiny 串流 / 打斷行為建模
    seed: 46
    precision: bf16
    train_steps: 24000
    save_interval: 3000
    log_interval: 50
    validation_interval: 1200
    resume_from: checkpoints/stage4/last
    use_deepspeed: true
    accelerate_config: configs/deepspeed.json
    optimizer:
      lr_adapter: 0.0001
      lr_audio_head: 0.0001
      lr_text_head: 0.0001
      lr_interrupt_head: 0.0001
      lr_qwen_lora: 0.00002
      warmup_steps: 1000
      scheduler: cosine
      min_lr_ratio: 0.1
    batching:
      max_tokens_per_device: 3200
      gradient_accumulation: 16
      dataloader_num_workers: 8
      max_context: 4096
      bucket_by_length: true
    train_manifests:
      - manifest: data/manifests/tiny_dialog.jsonl
        weight: 0.3
        type: spoken_dialog
      - manifest: data/manifests/tiny_synth_tts.jsonl
        weight: 0.3
        type: synth
      - manifest: data/manifests/tiny_duplex.jsonl
        weight: 0.3
        type: duplex_synthetic
      - manifest: data/manifests/tiny_asr.jsonl
        weight: 0.1
        type: asr
    eval_manifests:
      - manifest: data/manifests/tiny_duplex_dev.jsonl
        weight: 1.0
        type: duplex_synthetic
    additional_heads:
      interruptibility:
        type: binary
        loss_weight: 0.3
    latency_training:
      random_future_mask_ms: [160, 320]
      chunk_tokens: 48
      kd_latency_weight: 0.2
    mixing:
      datasets:
        - name: spoken_dialog
          weight: 0.25
        - name: asr
          weight: 0.1
        - name: tts
          weight: 0.15
        - name: synth
          weight: 0.3
        - name: duplex_synthetic
          weight: 0.2
      duplex_generation:
        barge_in_probability: 0.4
        noise_overlay_probability: 0.25
    validation_metrics: [wer, latency_ms, barge_in_success, false_interrupt_rate]

  stage6:
    description: Stage 6 - Semantic distillation and safety alignment
    seed: 47
    precision: bf16
    train_steps: 40000
    save_interval: 4000
    log_interval: 50
    validation_interval: 1000
    resume_from: checkpoints/stage5/last
    use_deepspeed: true
    accelerate_config: configs/deepspeed.json
    optimizer:
      lr_adapter: 0.00005
      lr_audio_head: 0.00005
      lr_text_head: 0.00005
      lr_qwen_lora: 0.00001
      warmup_steps: 1000
      scheduler: cosine
      min_lr_ratio: 0.05
    loss:
      objectives:
        - name: text_ce
          weight: 0.7
        - name: audio_codebook_ce
          weight: 0.2
        - name: distill_kl
          weight: 0.2
        - name: safety_sft
          weight: 0.3
    teacher:
      model_id: Qwen/Qwen2.5-72B-Instruct
      temperature: 1.0
      kl_target: 0.05
      max_tokens: 4096
    safety:
      template_manifest: data/manifests/safety_templates.jsonl
      refusal_label: <SAFETY_REFUSAL>
      audit_ruleset: configs/safety.yml
    batching:
      max_tokens_per_device: 2600
      gradient_accumulation: 16
      dataloader_num_workers: 8
      max_context: 4096
    validation_metrics: [perplexity, safety_precision, safety_recall, refusal_false_positive]

audio_decoder:
  description: Stage 3 - Audio decoder (token -> waveform)
  seed: 44
  precision: fp16
  train_steps: 200000
  save_interval: 5000
  log_interval: 100
  validation_interval: 2000
  model:
    type: nar_transformer
    d_model: 896
    num_layers: 18
    num_heads: 8
    ffn_dim: 3584
    dropout: 0.1
    codebooks: 4
    codes_per_book: 8192
    chunk_tokens: 200
  optimizer:
    name: adamw
    lr: 0.0002
    betas: [0.9, 0.98]
    weight_decay: 0.01
    warmup_steps: 2000
    scheduler: cosine
  batching:
    batch_size: 128
    gradient_accumulation: 4
    dataloader_num_workers: 8
  augmentation:
    spec_augment: 0.1
    noise_mix_prob: 0.1
  loss:
    objectives:
      - name: code_ce
        weight: 0.7
      - name: mel_l1
        weight: 0.2
      - name: stft_mag
        weight: 0.1
  validation_metrics: [stoi, pesq, dnsmos]
